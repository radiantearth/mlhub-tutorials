{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f7fd9c",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# 2021 NASA Harvest Rwanda Baseline Model\n",
    "\n",
    "This notebook walks you through the steps to create a baseline field delineation model for detecting boundaries from Sentinel-2 time-series satellite imagery using a spatio-temporal U-Net model on the [2021 NASA Harvest Rwanda dataset](https://mlhub.earth/data/nasa_rwanda_field_boundary_competition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed libraries\n",
    "import getpass\n",
    "import glob\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from radiant_mlhub import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet\n",
    "\n",
    "from pathlib import Path\n",
    "from random import choice\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import List, Any, Callable, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55d4fe",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "### Create an API Key\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [mlhub.earth/profile](mlhub.earth/profile). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key(s), which you will need. Do not *share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details).\n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "*If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb373133",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLHUB_API_KEY'] = 'YOUR API KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0187a7",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "Now that your API credentials have been specified, we will download the data from the [Radiant-MLHub](https://mlhub.earth/) API through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset id and fetch the dataset\".\n",
    "dataset_id = 'nasa_rwanda_field_boundary_competition'\n",
    "dataset = Dataset.fetch(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.download(if_exists='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image snapshot dimensions and the RBGA Bands\n",
    "IMG_WIDTH = 256 \n",
    "IMG_HEIGHT = 256 \n",
    "IMG_CHANNELS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ddebe",
   "metadata": {},
   "source": [
    "### The Training Data\n",
    "We have two sets of data: the `train` and `test` datasets.  \n",
    "\n",
    "For model development purposes, we will use the training source imagery and label set (`source_train` and `labels_train`) for the model development and use the test set (`source_test`) for model prediction/evaluation.\n",
    "\n",
    "This saves us memory from training more imagery in our model as well as prevents the model from overfitting. \n",
    "Having a separate test dataset which the model hasn't previously seen for prediction also helps us observe the model's results and confirm if they are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_items = f\"{dataset_id}/{dataset_id}_source_train\"\n",
    "train_label_items = f\"{dataset_id}/{dataset_id}_labels_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564932",
   "metadata": {},
   "source": [
    "### Create List of Unique Training IDs\n",
    "The train source files follow the convention `nasa_rwanda_field_boundary_competition_source_train_ID_YYYY_MM`. We will obtain a list of all the unique IDs for each field where a unique ID is of the format `ID_YYYY_MM`. This makes it easier to load the train dataset.\n",
    "\n",
    "The corresponding training label files do not inclue any date in the name of the file. Therefore, the training label files follow the convention `nasa_rwanda_field_boundary_competition_label_train_ID`. By striping the `YYYY_MM` component from out from our unique ID (going from `ID_YYYY_MM` to `simply ID`), we can use the unique ID to obtain the corresponding label file for the source imagery.\n",
    "\n",
    "We will use the function `clean_string` below to get the list of unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    extract the tile id and timestamp from a source image folder\n",
    "    e.g extract 'ID_YYYY_MM' from 'nasa_rwanda_field_boundary_competition_source_train_ID_YYYY_MM'\n",
    "    \"\"\"\n",
    "    s = s.replace(f\"{dataset_id}_source_\", '').split('_')[1:]\n",
    "    return '_'.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcde75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tiles = [clean_string(s) for s in next(os.walk(train_source_items))[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21d0c0",
   "metadata": {},
   "source": [
    "### Normalize the Source Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbd418",
   "metadata": {},
   "source": [
    "We will need to normalize our source imagery to remove any pixel values that are greater than 255. We apply the min-max normalisation calculation for each image: $${\\text{all pixel values - minimum pixel value} \\over \\text{maximum pixel value - minimum pixel value}}$$\n",
    "\n",
    "If we don't, we will be working with distorted images that would add no value to the model during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e534f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(\n",
    "    array: np.ndarray\n",
    "):\n",
    "    \"\"\" normalise image to give a meaningful output \"\"\"\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return (array - array_min) / (array_max - array_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad06f4",
   "metadata": {},
   "source": [
    "### Augmentation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2060bd",
   "metadata": {},
   "source": [
    "Here, data augmentation will be used to populate the model with data using our normalised images for training to obtain even more accurate results.\n",
    "\n",
    "We will employ the following data augmentation techniques on the dataset:\n",
    "\n",
    "- Rotation\n",
    "\n",
    "- Flipping\n",
    "\n",
    "- Blurring\n",
    "\n",
    "These techniques were developed by thanks to the [radix-ai](https://github.com/radix-ai/agoro-field-boundary-detector).\n",
    "\n",
    "We will input the post-normalization 4-band stack of each image. The 4 bands in this stack will include:\n",
    "\n",
    "- B01 (Ultra Blue (Coastal and Aerosol))\n",
    "\n",
    "- B02 (Blue)\n",
    "\n",
    "- B03 (Green) and\n",
    "\n",
    "- B04 (Red)\n",
    "\n",
    "The results of a random source image and its associated label can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = random.choice(train_tiles)\n",
    "print(tile)\n",
    "bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "bd1_array = bd1.read(1)\n",
    "bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "bd2_array = bd2.read(1)\n",
    "bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "bd3_array = bd3.read(1)\n",
    "bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "bd4_array = bd4.read(1)\n",
    "b01_norm = normalize(bd1_array)\n",
    "b02_norm = normalize(bd2_array)\n",
    "b03_norm = normalize(bd3_array)\n",
    "b04_norm = normalize(bd4_array)\n",
    "\n",
    "field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile.split('_')[0]}/raster_labels.tif\").read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85bdb4",
   "metadata": {},
   "source": [
    "We have the following functions that perform the augmentation pieces:\n",
    "\n",
    "- `t_linear`: performs no augmentation and maintains the same copy of the imagery.\n",
    "\n",
    "- `t_rotation`: performs rotation on the imagery (e.g by 90 degrees) with multiple rotated copies.\n",
    "\n",
    "- `t_flip`: performs flipping on the imagery (e.g diagonally, horizontally and vertically) with multiple copies.\n",
    "\n",
    "- `t_blur`: performs blurring on the imagery using a Gaussian filter with multiple copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206812fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_linear(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    _: int = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply a linear (i.e. no) transformation and save.\"\"\"\n",
    "    return field, mask\n",
    "\n",
    "def t_rotation(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    rot: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Rotate the data.\"\"\"\n",
    "    assert rot in range(0, 3 + 1)\n",
    "    for _ in range(rot):\n",
    "        field = np.rot90(field)\n",
    "        mask = np.rot90(mask)\n",
    "    return field, mask\n",
    "\n",
    "def t_flip(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    idx: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Flip the data.\"\"\"\n",
    "    assert idx in range(0, 2 + 1)\n",
    "    if idx == 0: \n",
    "        # Diagonal\n",
    "        field = np.rot90(np.fliplr(field))\n",
    "        mask = np.rot90(np.fliplr(mask))\n",
    "    if idx == 1: \n",
    "        # Horizontal\n",
    "        field = np.flip(field, axis=0)\n",
    "        mask = np.flip(mask, axis=0)\n",
    "    if idx == 2: \n",
    "        # Vertical\n",
    "        field = np.flip(field, axis=1)\n",
    "        mask = np.flip(mask, axis=1)\n",
    "    return field, mask\n",
    "\n",
    "def t_blur(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    sigma: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Blur the image by applying a Gaussian filter.\"\"\"\n",
    "    assert 0 <= sigma <= 10\n",
    "    sigma_f = 1.0 + (sigma / 10)\n",
    "    field = np.copy(field)\n",
    "    for i in range(3):\n",
    "        field[:, :, i] = gaussian_filter(field[:, :, i], sigma=sigma_f)\n",
    "    return field, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec9c31",
   "metadata": {},
   "source": [
    "### Helper function for displaying RGB bands of an image\n",
    "\n",
    "The function `show_image` below allows us to display an RGB image. Below, the `add_subplot` attributes define the left and the right side of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(field:np.ndarray, mask:np.ndarray): \n",
    "    \"\"\"\n",
    "    Show the field and corresponding mask.\n",
    "    We will just display the RGB bands for simplicity\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax1.imshow(field[:,:,0:3])\n",
    "    plt.gray()\n",
    "    ax2.imshow(mask)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd31c5",
   "metadata": {},
   "source": [
    "### Display Augmented Image Effects\n",
    "Let's display the image with the augmented effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239f568",
   "metadata": {},
   "source": [
    "1. The un-augmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(field, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3c94a",
   "metadata": {},
   "source": [
    "2. The rotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f36ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_rotation(field, mask, rot=1)\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9dcf2",
   "metadata": {},
   "source": [
    "3. The flipped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdefc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_flip(field, mask, idx=0)\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3807a",
   "metadata": {},
   "source": [
    "4. The blurred image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f46a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_blur(field, mask, sigma=5)\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3345bc3",
   "metadata": {},
   "source": [
    "We will use the `generate` and `main` functions below to generate and save data augmented images for every source image and its associated label in our training data. We will save the resulting data as pickle `.pkl` files.\n",
    "\n",
    "We will need to pass into the function: the source image, label and the path to save the augmented images in our directory and a prefix which indicates the id of the imagery. In our case, this is the unique ID we got earlier excluding the timestamp i.e `XX` from `XX_YYYY_MM_DD`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb229002",
   "metadata": {},
   "source": [
    "### Apply Augmentation & Save Images\n",
    "\n",
    "Now, we need to apply these augmentation techniques to all the source images and their associated labels in our training dataset. \n",
    "\n",
    "To generate and save the augmented version of this data, we will use the functions `generate` and `main` below. The resulting data will be saved as `.pkl` (aka pickle) files.\n",
    "\n",
    "The parameters of these functions are:\n",
    "\n",
    "- the source image\n",
    "\n",
    "- the associated label \n",
    "\n",
    "- the path to save the augmented images in our directory \n",
    "\n",
    "- the image's ID\n",
    "\n",
    "  - As we discussed above, the ID is the `XX` portion of the `XX_YYYY_MM_DD` portion of the file naming convention.\n",
    "\n",
    "   \n",
    "\n",
    "The main function will be called in the sequence below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    write_folder: Path,\n",
    "    prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate data augmentations of the provided field and corresponding mask which includes:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "    :param field: Input array of the field to augment\n",
    "    :param mask: Input array of the corresponding mask to augment\n",
    "    :param write_folder: Folder (path) to write the results (augmentations) to\n",
    "    :param prefix: Field-specific prefix used when writing the augmentation results\n",
    "    \"\"\"\n",
    "    # Generate transformations\n",
    "    # Dummy data. will be replaced\n",
    "    f, m = [0,1,2,3], [0,1,2,3]\n",
    "    # No augmentation\n",
    "    f[0],m[0] = t_linear(field, mask)\n",
    "    # Rotation\n",
    "    f[1],m[1] = t_rotation(field, mask, rot=1)\n",
    "    # Flipping\n",
    "    f[2],m[2] = t_flip(field, mask, idx=0)\n",
    "    # Blurring\n",
    "    f[3],m[3] = t_blur(field, mask, sigma=5)\n",
    "    for i in range(len(f)):\n",
    "        with open(write_folder +'/'+ f\"fields/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(f[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(write_folder +'/'+ f\"masks/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(m[i], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c76a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    field: List[np.ndarray],\n",
    "    mask: List[np.ndarray],\n",
    "    prefix: List[str],\n",
    "    write_folder: Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save data augmentations for all the fields and corresponding masks with the following:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "     - Gamma filter (brightness)\n",
    "    :param fields: Fields to augment\n",
    "    :param masks: Corresponding masks to augment\n",
    "    :param prefixes: Field-specific prefixes corresponding each field\n",
    "    :param write_folder: Path to write the results (augmentations) to\n",
    "    \"\"\"\n",
    "    generate(\n",
    "        field=field,\n",
    "        mask=mask,\n",
    "        prefix=prefix,\n",
    "        write_folder=write_folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c9c5a",
   "metadata": {},
   "source": [
    "Here, we will use the `main` function which contains the `generate` function for saving the augmented images for all source images and labels.\n",
    "\n",
    "The following sequence happens for each tile in our train tiles in the block of code below:\n",
    "\n",
    "1. Get the source images for the four bands (B01, B02, B03, B04) using `rasterio`.\n",
    "\n",
    "2. Normalise the bands using the normalize function to obtain non-distorted images which make sense for modelling.\n",
    "\n",
    "3. Obtain the prefix id (`XX`) of the image and corresponding timestamp (`YYYY_MM`) from the tile.\n",
    "\n",
    "4. Stack the normalised bands on top of each other using `np.dstack`.\n",
    "\n",
    "5. Use the timestamp as the folder name for our augmented images and the prefix will be used for the filename i.e `/augmented_data/{timestamp}/prefix_{augmentation_id}` (e.g `/augmented_data/2021_01/12_01.pkl`) where `augmentation_id` is the allocated id for the augmented image.\n",
    "\n",
    "6. Run the augmentation on source images and respective labels.\n",
    "\n",
    "7. Save results from #6 into the file formats and directories described in #5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation effects to training set\n",
    "for tile in train_tiles:\n",
    "    bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "    bd1_array = bd1.read(1)\n",
    "    bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "    bd2_array = bd2.read(1)\n",
    "    bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "    bd3_array = bd3.read(1)\n",
    "    bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "    bd4_array = bd4.read(1)\n",
    "    b01_norm = normalize(bd1_array)\n",
    "    b02_norm = normalize(bd2_array)\n",
    "    b03_norm = normalize(bd3_array)\n",
    "    b04_norm = normalize(bd4_array)\n",
    "\n",
    "    # XX_YYYY_MM where XX is the training file id and YYYY_MM is the timestamp\n",
    "    ids_list  = tile.split('_')\n",
    "    tile_id   = ids_list[0]\n",
    "    timestamp = f\"{ids_list[1]}_{ids_list[2]}\"\n",
    "\n",
    "    field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "    mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile_id}/raster_labels.tif\").read(1) \n",
    "\n",
    "    # Create a folder for the augmented images\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/fields\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/fields\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/masks\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/masks\")\n",
    "\n",
    "    main(\n",
    "        # Applying augmentation effects\n",
    "        field  = field,\n",
    "        mask   = mask,\n",
    "        prefix = tile_id,\n",
    "        write_folder = f\"./augmented_data/{timestamp}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all timestamps\n",
    "timestamps = next(os.walk(f\"./augmented_data\"))[1]\n",
    "# Get all augmented tile ids. can just use one timestamp\n",
    "augmented_files = next(os.walk(f\"./augmented_data/{timestamps[0]}/fields\"))[2]\n",
    "# Time-series image\n",
    "X = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32)\n",
    "# Mask for each scene\n",
    "y = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n",
    "i = 0\n",
    "for file in augmented_files:\n",
    "    idx = 0\n",
    "    # Id without .pkl extension\n",
    "    augmented_id = file.split('.pkl')[0]\n",
    "    temporal_fields = []\n",
    "    for timestamp in timestamps:\n",
    "        with open(f\"./augmented_data/{timestamp}/fields/{augmented_id}.pkl\", 'rb') as field:\n",
    "            field = pickle.load(field) \n",
    "        X[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx += IMG_CHANNELS\n",
    "    with open(f\"./augmented_data/{timestamp}/masks/{augmented_id}.pkl\", 'rb') as mask:\n",
    "        mask = pickle.load(mask)\n",
    "    y[i] = mask.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88dd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "random.randint(0, len(augmented_files))\n",
    "random_image = random.randint(0, len(augmented_files)-1)\n",
    "show_image(X[random_image][:,:,0:3], y[random_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf9520",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The U-Net model has been selected in this training for its impressive results over multiple domains in image segmentation.\n",
    "We will employ a ResNet34 backbone with our spatio-temporal U-Net model.\n",
    "\n",
    "This model uses our 24 channels (6 timestamps * 4 bands per timestamp) and generates the predicted field boundary per scene.\n",
    "\n",
    "We will also use an 80:20 train:validation set split for model training.\n",
    "\n",
    "Since this is a binary segmentation problem (field boundary or no field boundary), we will use the `binary cross_entropy` loss.\n",
    "\n",
    "We will also use our GPU device for faster model training with better computing power.\n",
    "\n",
    "Batch normalisation makes training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02486ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate GPU resource for model training\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493266c6",
   "metadata": {},
   "source": [
    "The model's architecture below was tweaked from [sustainlab-group](https://github.com/sustainlab-group/ParcelDelineation/blob/master/models/unet.py) using our number of bands and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (256,256,4)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_scheduler(epoch):\n",
    "    lr = 1e-4\n",
    "    '''\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    '''\n",
    "    print(\"Set Learning Rate : {}\".format(lr))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643843fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 24\n",
    "input_shape = (256,256,num_channels)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582bf5a",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15275933",
   "metadata": {},
   "source": [
    "For the model, we will make use of two key metrics: **Recall** and **F1-score**.\n",
    "\n",
    "**Recall** evaluates how much of the field boundaries which were labelled were actually predicted correctly while the **F1-score** combines the precision and recall by evaluating the harmonic mean.\n",
    "\n",
    "Recall is calculated as the number of true positives divided by the total number of true positives and false negatives.\n",
    "\n",
    "$${\\text{True Positives}\\over\\text{True Positives + False Negatives}}$$\n",
    "\n",
    "To understand F1-score, first we will define what precision is.\n",
    "\n",
    "Precision is calculated as the number of true positives divided by the total number of true positives and false positives.\n",
    "\n",
    "$${\\text{True Positives}\\over\\text{True Positives + False Positives}}$$\n",
    "\n",
    "Once precision and recall have been calculated for a binary or multiclass classification problem, the two scores can be combined into the calculation of the F1-score.\n",
    "\n",
    "The traditional F1-score is calculated as follows:\n",
    "\n",
    "$${\\text{2 * Precision * Recall}\\over\\text{Precision + Recall}}$$\n",
    "\n",
    "**NOTE** that the recall is the more important metric for this case as we are mostly concerned about the retrieved field boundaries out of the labelled field boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7c08e",
   "metadata": {},
   "source": [
    "This [link](https://datascience.stackexchange.com/a/45166) gives a really good implementation on the recall and F1 metrics using Keras. Modifying it a bit into the functions below for our model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e689d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "def f1(y_true, y_pred):\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall_   = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall_)/(precision+recall_+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b4866",
   "metadata": {},
   "source": [
    "The U-Net model is initialized with weights from a pre-trained `imagenet` encoder.\n",
    "The `binary_crossentropy` loss is used with the recall and F1 metrics.\n",
    "80% of the total data input is allocated to the training set while 20% is allocated to the validation set.\n",
    "\n",
    "For the sake of compatibility between `tensorflow` and `keras`, eager execution of the `tf.function` decorator function for model training is enabled.\n",
    "\n",
    "The model is then trained for about `200` epochs to fit well with the data and extract useful information out of it.\n",
    "\n",
    "It is important to observe the model's performance during training to avoid underfitting and especially, overfitting.\n",
    "Not using enough epochs doesn't allow the model to learn enough about the dataset, which will lead to underfitting. On the other hand, too many epochs makes the model understand too much about the data, which will lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None \n",
    "model_unet = Unet(BACKBONE, encoder_weights='imagenet')\n",
    "new_model = keras.models.Sequential()\n",
    "new_model.add(Conv2D(3, (1,1), padding='same', activation='relu', input_shape=input_shape))\n",
    "new_model.add(model_unet)\n",
    "model = new_model \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate_scheduler(0)),\n",
    "              metrics=[recall,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e51a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train, y=y_train,\n",
    "              validation_data=(x_val, y_val),\n",
    "              steps_per_epoch = len(x_train)//batch_size,\n",
    "              validation_steps = len(x_val)//batch_size,\n",
    "              batch_size=batch_size, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b78ee",
   "metadata": {},
   "source": [
    "We will then save the model into the directory we are working in, so that we can load for model prediction on the test dataset.\n",
    "\n",
    "We named the model `unet_model` and it will be stored in the same relative directory we are working on.\n",
    "\n",
    "We will then load the model with the defined metrics for prediction on test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./unet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"./unet_model.h5\", custom_objects ={\"recall\":sm.metrics.Recall(threshold=0.5), \"f1\": f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23753d38",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329471ba",
   "metadata": {},
   "source": [
    "The final step is to apply the model prediction on the test dataset.\n",
    "\n",
    "We will extract the unique ids for the test dataset, just like we did with the train dataset and store them into `test_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_test = f\"{dataset_id}/{dataset_id}_source_test\"\n",
    "test_tiles = [clean_string(s) for s in next(os.walk(source_test))[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde8961",
   "metadata": {},
   "source": [
    "We will then obtain `test_tile_ids` which contains the field overall id excluding the timestamps e.g `10` from `10_2021_12`. Since we already have the list of timestamps in our variable `timestamps`, we only need the field id.\n",
    "\n",
    "We will then extract the source images (normalising, stacking bands) and store them into `X_test` while `loaded_tiles` tracks the id for each tile that we load in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a880ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_ids = set()\n",
    "for tile in test_tiles:\n",
    "    test_tile_ids.add(tile.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29312c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.empty((len(test_tile_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32)\n",
    "i = 0\n",
    "loaded_tiles = []\n",
    "for tile_id in test_tile_ids:\n",
    "    idx = 0\n",
    "    for timestamp in timestamps:\n",
    "        bd1 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B01.tif\")\n",
    "        bd1_array = bd1.read(1)\n",
    "        bd2 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B02.tif\")\n",
    "        bd2_array = bd2.read(1)\n",
    "        bd3 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B03.tif\")\n",
    "        bd3_array = bd3.read(1)\n",
    "        bd4 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B04.tif\")\n",
    "        bd4_array = bd4.read(1)\n",
    "        b01_norm = normalize(bd1_array)\n",
    "        b02_norm = normalize(bd2_array)\n",
    "        b03_norm = normalize(bd3_array)\n",
    "        b04_norm = normalize(bd4_array)\n",
    "        \n",
    "        field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "        X_test[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx+=IMG_CHANNELS\n",
    "    # Track order test tiles are loaded into X to make sure tile id matches \n",
    "    loaded_tiles.append(str(tile_id).zfill(2)) \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9b70d",
   "metadata": {},
   "source": [
    "We will then use the model to predict the field boundaries from the time-series source imagery.\n",
    "The model's prediction is probabilistic i.e from 0 to 1, where 0 signifies that the model believes no field boundary exist for that particular pixel and 1 signifies the model is confident a field boundary definitely exists for the pixel. That is, a model prediction of 0.3 for a pixel signifies the model believes there is a 30% chance the pixel indicates a field boundary.\n",
    "\n",
    "Since the label images are either 0 or 1 (a field boundary doesn't exist or it does) for each pixel, We have to choose a threshold where probabilities below that threshold represent 0 (not field boundary) and probabilities above it represent 1 (field boundary). \n",
    "\n",
    "In this case, we will choose 0.5 as a fair threshold, which is seen in the below code.\n",
    "\n",
    "Also, we will reshape the prediction to fit our standard `IMG_HEIGHT * IMG_WIDTH` dimension e.g 256 x 256.\n",
    "\n",
    "We will then save the prediction into a dictionary where the key is the field id which we stored in `loaded_tiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dictionary = {}\n",
    "for i in range(len(test_tile_ids)):\n",
    "    model_pred = model.predict(np.expand_dims(X_test[i], 0))\n",
    "    model_pred = model_pred[0]\n",
    "    model_pred = (model_pred >= 0.5).astype(np.uint8)\n",
    "    model_pred = model_pred.reshape(IMG_HEIGHT, IMG_WIDTH)\n",
    "    predictions_dictionary.update([(str(loaded_tiles[i]), pd.DataFrame(model_pred))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64829ce",
   "metadata": {},
   "source": [
    "We will then save the predictions as a `.csv` file as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33186bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for key, value in predictions_dictionary.items():\n",
    "    ftd = value.unstack().reset_index().rename(columns={'level_0': 'row', 'level_1': 'column', 0: 'label'})\n",
    "    ftd['tile_row_column'] = f'Tile{key}_' + ftd['row'].astype(str) + '_' + ftd['column'].astype(str)\n",
    "    ftd = ftd[['tile_row_column', 'label']]\n",
    "    dfs.append(ftd)\n",
    "\n",
    "sub = pd.concat(dfs)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f\"./harvest_sample_submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1456b81e0739e370391a1194b27bc839a9f5597c29befb4a579ec9fcd0f2acde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
