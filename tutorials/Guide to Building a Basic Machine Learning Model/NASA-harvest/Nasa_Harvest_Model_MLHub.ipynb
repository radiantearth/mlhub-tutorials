{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f7fd9c",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# 2021 NASA Harvest Rwanda Baseline Model\n",
    "\n",
    "This notebook walks you through the steps to create a baseline field delineation model for detecting boundaries from sentinel-2 time-series satellite imagery using a spatio-temporal U-Net model on the 2021 NASA Harvest Rwanda dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337cc89",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "All the dependencies for this notebook are included in the `requirements.txt` file included in this folder.\n",
    "\n",
    "Make a pip install on the requirements file to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed libraries\n",
    "import getpass\n",
    "import glob\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from radiant_mlhub import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet\n",
    "\n",
    "from pathlib import Path\n",
    "from random import choice\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import List, Any, Callable, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c6004",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f87c93c",
   "metadata": {},
   "source": [
    "We will download the data from `Radiant-MLHub` and extract it.\n",
    "All we need to do is specify the `dataset_id` to download using the API key which can be obtained from [mlhub.earth](https://mlhub.earth).\n",
    "\n",
    "If the dataset exists in our directory and want to re-download anyway, it will overwrite our existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append your MLHUB_API_KEY after this cell is executed to download dataset\n",
    "dataset_id = 'nasa_rwanda_field_boundary_competition'\n",
    "os.environ['MLHUB_API_KEY'] =  getpass.getpass(prompt=\"MLHub API Key: \")\n",
    "dataset = Dataset.fetch(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.download(if_exists='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e40e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image snapshot dimensions\n",
    "IMG_WIDTH = 256 \n",
    "IMG_HEIGHT = 256 \n",
    "IMG_CHANNELS = 4 #we have the rgba bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ddebe",
   "metadata": {},
   "source": [
    "We have two sets of data: the `train` and `test` dataset, each having a list of file ids belonging to them.\n",
    "For model development purposes, we will use the training set(`source_train` and `labels_train` which are the train source imagery and labels respectively) and use the test set for model prediction/evaluation.\n",
    "\n",
    "This saves us memory from training more imagery in our model as well as prevents the model from overfitting. Having a separate test dataset which the model hasn't previously seen for prediction also helps give a fair assessment of the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_items = f\"{dataset_id}/{dataset_id}_source_train\"\n",
    "train_label_items = f\"{dataset_id}/{dataset_id}_labels_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564932",
   "metadata": {},
   "source": [
    "The train source files follow the convention `nasa_rwanda_field_boundary_competition_source_train_ID_YYYY_MM`. We will obtain a list of all the unique IDs for each field where a unique ID is of the format `ID_YYYY_MM`. This makes it easier to load the train dataset.\n",
    "\n",
    "Since the train label files follow the convention `nasa_rwanda_field_boundary_competition_label_train_ID`, we can just strip the `YYYY_MM` id from our unique ID to obtain the corresponding label file for the source imagery.\n",
    "Thus, the unique ID is very useful for obtaining the source and label files for a field in an instance of time.\n",
    "\n",
    "We will use the function `clean_string` below to get the list of unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    extract the tile id and timestamp from a source image folder\n",
    "    e.g extract 'ID_YYYY_MM' from 'nasa_rwanda_field_boundary_competition_source_train_ID_YYYY_MM'\n",
    "    \"\"\"\n",
    "    s = s.replace(f\"{dataset_id}_source_\", '').split('_')[1:]\n",
    "    return '_'.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcde75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tiles = [clean_string(s) for s in next(os.walk(train_source_items))[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e56a84",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbd418",
   "metadata": {},
   "source": [
    "Our source images have pixel values > 255, hence we need to apply normalisation on our images to generate a normalised image. We apply the min-max normalisation for this which is simply, for each image: $${\\text{all pixel values - minimum pixel value} \\over \\text{maximum pixel value - minimum pixel value}}$$\n",
    "\n",
    "This ensures that our image is uniformly distributed and displays the \"normal\" image which makes sense. If we don't, we will be working with distorted images that would add no value to the model during model training.                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e534f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(\n",
    "    array: np.ndarray\n",
    "):\n",
    "    \"\"\" normalise image to give a meaningful output \"\"\"\n",
    "    array_min, array_max = array.min(), array.max()\n",
    "    return (array - array_min) / (array_max - array_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2060bd",
   "metadata": {},
   "source": [
    "In this notebook, we will perform data augmentation on our normalised images. This will be used to populate the model with data for training to obtain even more accurate results.\n",
    "\n",
    "We will employ the following data augmentation techniques on the dataset:\n",
    "- Rotation, Flipping, Blurring.\n",
    "\n",
    "These techniques were thanks to the radix-ai GitHub repository, which can be accessed [here](https://github.com/radix-ai/agoro-field-boundary-detector).\n",
    "\n",
    "We will use 4 bands per image (seen below) with the bands stacked/concatenated on top of each other after normalisation:\n",
    "- B01 (Ultra Blue (Coastal and Aerosol))\n",
    "- B02 (Blue)\n",
    "- B03 (Green) and\n",
    "- B04 (Red)\n",
    "\n",
    "We will observe the results on a random source image and its associated label below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the 4 bands of the image\n",
    "tile = random.choice(train_tiles) #choose a random tile\n",
    "print(tile)\n",
    "bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "bd1_array = bd1.read(1)\n",
    "bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "bd2_array = bd2.read(1)\n",
    "bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "bd3_array = bd3.read(1)\n",
    "bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "bd4_array = bd4.read(1)\n",
    "b01_norm = normalize(bd1_array)\n",
    "b02_norm = normalize(bd2_array)\n",
    "b03_norm = normalize(bd3_array)\n",
    "b04_norm = normalize(bd4_array)\n",
    "\n",
    "field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile.split('_')[0]}/raster_labels.tif\").read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85bdb4",
   "metadata": {},
   "source": [
    "We have the following functions that perform the augmentation pieces:\n",
    "\n",
    "-`t_linear`: performs no augmentation and maintains the same copy of the imagery.\n",
    "\n",
    "-`t_rotation`: performs rotation on the imagery (e.g by 90 degrees) with multiple rotated copies.\n",
    "\n",
    "-`t_flip`: performs flipping on the imagery (e.g diagonally, horizontally and vertically) with multiple copies.\n",
    "\n",
    "-`t_blur`: performs blurring on the imagery using a Gaussian filter with multiple copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206812fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/radix-ai/agoro-field-boundary-detector/tree/master/src/agoro_field_boundary_detector\n",
    "def t_linear(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    _: int = 0,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply a linear (i.e. no) transformation and save.\"\"\"\n",
    "    return field, mask\n",
    "\n",
    "def t_rotation(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    rot: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Rotate the data.\"\"\"\n",
    "    assert rot in range(0, 3 + 1)\n",
    "    for _ in range(rot):\n",
    "        field = np.rot90(field)\n",
    "        mask = np.rot90(mask)\n",
    "    return field, mask\n",
    "\n",
    "def t_flip(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    idx: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Flip the data.\"\"\"\n",
    "    assert idx in range(0, 2 + 1)\n",
    "    if idx == 0:  # Diagonal\n",
    "        field = np.rot90(np.fliplr(field))\n",
    "        mask = np.rot90(np.fliplr(mask))\n",
    "    if idx == 1:  # Horizontal\n",
    "        field = np.flip(field, axis=0)\n",
    "        mask = np.flip(mask, axis=0)\n",
    "    if idx == 2:  # Vertical\n",
    "        field = np.flip(field, axis=1)\n",
    "        mask = np.flip(mask, axis=1)\n",
    "    return field, mask\n",
    "\n",
    "def t_blur(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    sigma: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Blur the image by applying a Gaussian filter.\"\"\"\n",
    "    assert 0 <= sigma <= 10\n",
    "    sigma_f = 1.0 + (sigma / 10)\n",
    "    field = np.copy(field)\n",
    "    for i in range(3):\n",
    "        field[:, :, i] = gaussian_filter(field[:, :, i], sigma=sigma_f)\n",
    "    return field, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for displaying RGB bands of an image\n",
    "def show_image(field:np.ndarray, mask:np.ndarray): \n",
    "    \"\"\"\n",
    "    Show the field and corresponding mask.\n",
    "    We will just display the RGB bands for simplicity\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax1 = fig.add_subplot(121)  # left side\n",
    "    ax2 = fig.add_subplot(122)  # right side\n",
    "    ax1.imshow(field[:,:,0:3])  # rgb band\n",
    "    plt.gray()\n",
    "    ax2.imshow(mask)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd31c5",
   "metadata": {},
   "source": [
    "Finally, we will display the image with the augmented effects using the `show_image` helper function as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(field, mask) #no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f36ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_rotation(field, mask, rot=1) #rotation\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdefc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_flip(field, mask, idx=0) #flipping\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f46a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,m = t_blur(field, mask, sigma=5) #blur\n",
    "show_image(f,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3345bc3",
   "metadata": {},
   "source": [
    "We will use the `generate` and `main` functions below to generate and save data augmented images for every source image and its associated label in our training data. We will save the resulting data as pickle `.pkl` files.\n",
    "\n",
    "We will need to pass into the function: the source image, label and the path to save the augmented images in our directory and a prefix which indicates the id of the imagery. In our case, this is the unique ID we got earlier excluding the timestamp i.e `XX` from `XX_YYYY_MM_DD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    field: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    write_folder: Path,\n",
    "    prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate data augmentations of the provided field and corresponding mask which includes:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "    :param field: Input array of the field to augment\n",
    "    :param mask: Input array of the corresponding mask to augment\n",
    "    :param write_folder: Folder (path) to write the results (augmentations) to\n",
    "    :param prefix: Field-specific prefix used when writing the augmentation results\n",
    "    \"\"\"\n",
    "    # Generate transformations\n",
    "    f, m = [0,1,2,3], [0,1,2,3] #dummy data. will be replaced\n",
    "    f[0],m[0] = t_linear(field, mask) #no augmentation\n",
    "    f[1],m[1] = t_rotation(field, mask, rot=1) #rotation\n",
    "    f[2],m[2] = t_flip(field, mask, idx=0) #flipping\n",
    "    f[3],m[3] = t_blur(field, mask, sigma=5) #blur\n",
    "    for i in range(len(f)):        \n",
    "        with open(write_folder +'/'+ f\"fields/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle: #zfill for string formatting\n",
    "            pickle.dump(f[i], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open(write_folder +'/'+ f\"masks/{str(prefix).zfill(2)}_{i}.pkl\", 'wb') as handle:\n",
    "            pickle.dump(m[i], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c76a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    field: List[np.ndarray],\n",
    "    mask: List[np.ndarray],\n",
    "    prefix: List[str],\n",
    "    write_folder: Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save data augmentations for all the fields and corresponding masks with the following:\n",
    "     - Linear (no) transformation\n",
    "     - Rotation\n",
    "     - Horizontal or vertical flip\n",
    "     - Gaussian filter (blur)\n",
    "     - Gamma filter (brightness)\n",
    "    :param fields: Fields to augment\n",
    "    :param masks: Corresponding masks to augment\n",
    "    :param prefixes: Field-specific prefixes corresponding each field\n",
    "    :param write_folder: Path to write the results (augmentations) to\n",
    "    \"\"\"\n",
    "    generate(\n",
    "        field=field,\n",
    "        mask=mask,\n",
    "        prefix=prefix,\n",
    "        write_folder=write_folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c9c5a",
   "metadata": {},
   "source": [
    "Here, we will use the `main` function which contains the `generate` function for saving the augmented images for all source images and labels.\n",
    "\n",
    "The following sequence happens for each tile in our train tiles in the block of code below:\n",
    "\n",
    "1. Get the source images for the four bands (B01, B02, B03, B04) using rasterio\n",
    "2. Normalise the bands using the `normalize` function to obtain non-distorted images which make sense for modelling.\n",
    "3. Obtain the prefix id of the image and corresponding timestamp from the tile.\n",
    "4. Stack the normalised bands on top of each other using `np.dstack`.\n",
    "5. The timestamp id will be used as the folder name for our augmented images and the prefix will be used for the filename i.e `/augemented_data/{timestamp}/prefix_{augmentation_id}` (e.g `/augmented_data/2021_01/12_01.pkl`) where `augmentation_id` is the allocated id for the augmented image.\n",
    "6. The augmentation is then carried out with source images and respective labels saved into the timestamped directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply augmentation effects to training set\n",
    "for tile in train_tiles:\n",
    "    bd1 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B01.tif\")\n",
    "    bd1_array = bd1.read(1)\n",
    "    bd2 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B02.tif\")\n",
    "    bd2_array = bd2.read(1)\n",
    "    bd3 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B03.tif\")\n",
    "    bd3_array = bd3.read(1)\n",
    "    bd4 = rio.open(f\"{train_source_items}/{dataset_id}_source_train_{tile}/B04.tif\")\n",
    "    bd4_array = bd4.read(1)\n",
    "    b01_norm = normalize(bd1_array)\n",
    "    b02_norm = normalize(bd2_array)\n",
    "    b03_norm = normalize(bd3_array)\n",
    "    b04_norm = normalize(bd4_array)\n",
    "\n",
    "    ids_list  = tile.split('_') # XX_YYYY_MM where XX is the training file id and YYYY_MM is the timestamp\n",
    "    tile_id   = ids_list[0]\n",
    "    timestamp = f\"{ids_list[1]}_{ids_list[2]}\"\n",
    "\n",
    "    field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "    mask  = rio.open(Path.cwd() / f\"{train_label_items}/{dataset_id}_labels_train_{tile_id}/raster_labels.tif\").read(1) \n",
    "\n",
    "    #create a folder for the augmented images\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/fields\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/fields\")\n",
    "    if not os.path.isdir(f\"./augmented_data/{timestamp}/masks\"):\n",
    "        os.makedirs(f\"./augmented_data/{timestamp}/masks\")\n",
    "\n",
    "    main( #applying augmentation effects\n",
    "        field  = field,\n",
    "        mask   = mask,\n",
    "        prefix = tile_id,\n",
    "        write_folder = f\"./augmented_data/{timestamp}\"\n",
    "    ) #approximately 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = next(os.walk(f\"./augmented_data\"))[1] #Get all timestamps\n",
    "augmented_files = next(os.walk(f\"./augmented_data/{timestamps[0]}/fields\"))[2] #Get all augmented tile ids. can just use one timestamp\n",
    "X = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32) #time-series image\n",
    "y = np.empty((len(augmented_files), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8) #mask for each scene\n",
    "i = 0\n",
    "for file in augmented_files:\n",
    "    idx = 0\n",
    "    augmented_id = file.split('.pkl')[0] #id without .pkl extension\n",
    "    temporal_fields = []\n",
    "    for timestamp in timestamps:\n",
    "        with open(f\"./augmented_data/{timestamp}/fields/{augmented_id}.pkl\", 'rb') as field:\n",
    "            field = pickle.load(field) \n",
    "        X[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx += IMG_CHANNELS\n",
    "    with open(f\"./augmented_data/{timestamp}/masks/{augmented_id}.pkl\", 'rb') as mask:\n",
    "        mask = pickle.load(mask)\n",
    "    y[i] = mask.reshape(IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88dd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(0, len(augmented_files)) #sanity check\n",
    "random_image = random.randint(0, len(augmented_files)-1)\n",
    "show_image(X[random_image][:,:,0:3], y[random_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf9520",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We decided to use U-Net as it has shown impressive results over multiple domains in image segmentation.\n",
    "We will employ a ResNet34 backbone with our spatio-temporal U-Net model.\n",
    "\n",
    "This uses our 24 channels (6 timestamps * 4 bands per timestamp) and generates the predicted field boundary per scene.\n",
    "\n",
    "We will also use an 80:20 train:validation set split for model training.\n",
    "\n",
    "Since this is a binary segmentation problem (field boundary or no field boundary), we will use the `binary cross_entropy` loss.\n",
    "\n",
    "We will also use our GPU device for faster model training with better computing power.\n",
    "\n",
    "Batch normalisation makes training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02486ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\") #activate GPU resource for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c631af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/sustainlab-group/ParcelDelineation/blob/master/models/unet.py\n",
    "def unet(pretrained_weights = None,input_size = (256,256,4)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_scheduler(epoch):\n",
    "    lr = 1e-4\n",
    "    '''\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    '''\n",
    "    print(\"Set Learning Rate : {}\".format(lr))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643843fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 24\n",
    "input_shape = (256,256,num_channels)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15275933",
   "metadata": {},
   "source": [
    "For the model, we will make use of two key metrics: **Recall** and **F1-score**.\n",
    "\n",
    "**Recall** evaluates how much of the field boundaries which were labelled were actually predicted as well while the **F1-score** combines the precision and recall by evaluating the harmonic mean.\n",
    "\n",
    "Recall is calculated as the number of true positives divided by the total number of true positives and false negatives.\n",
    "\n",
    "i.e $${\\text{True Positives}\\over\\text{True Positives + False Negatives}}$$\n",
    "\n",
    "To understand f1-score, first we will define what precision is.\n",
    "\n",
    "Precision is calculated as the number of true positives divided by the total number of true positives and false positives.\n",
    "\n",
    "i.e $${\\text{True Positives}\\over\\text{True Positives + False Positives}}$$\n",
    "\n",
    "Once precision and recall have been calculated for a binary or multiclass classification problem, the two scores can be combined into the calculation of the F-Score.\n",
    "\n",
    "The traditional F-Score is calculated as follows:\n",
    "\n",
    "$${\\text{2 * Precision * Recall}\\over\\text{Precision + Recall}}$$\n",
    "\n",
    "**NOTE** that the recall is the more important metric for this case as we are mostly concerned about the retrieved field boundaries out of the labelled field boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e689d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "def f1(y_true, y_pred):\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall_   = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall_)/(precision+recall_+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b4866",
   "metadata": {},
   "source": [
    "We will load the Unet model, using the `imagenet` encoder.\n",
    "As discussed, we're using the `binary_crossentropy` loss as well as the recall and f1 metrics. The validation set will have 20% of the total data input.\n",
    "\n",
    "We will enable eager execution of `tf.functions` to fit compatibility between `tensorflow` and `keras` for model training.\n",
    "\n",
    "We will train the model for about `200` epochs to allow the model fit well with the data and extract useful information out of it.\n",
    "\n",
    "It is important to observe the model performance during training to avoid underfitting and especially, overfitting.\n",
    "Using not enough epochs will not allow the model learn enough on the data set, hence underfitting and too many epochs makes the model understand too much about the data, hence overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None \n",
    "model_unet = Unet(BACKBONE, encoder_weights='imagenet')\n",
    "new_model = keras.models.Sequential()\n",
    "new_model.add(Conv2D(3, (1,1), padding='same', activation='relu', input_shape=input_shape))\n",
    "new_model.add(model_unet)\n",
    "model = new_model \n",
    "#sm.metrics.FScore(beta=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate_scheduler(0)),\n",
    "              metrics=[recall,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e51a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=x_train, y=y_train,\n",
    "              validation_data=(x_val, y_val),\n",
    "              steps_per_epoch = len(x_train)//batch_size,\n",
    "              validation_steps = len(x_val)//batch_size,\n",
    "              batch_size=batch_size, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b78ee",
   "metadata": {},
   "source": [
    "We will then save the model into our directory, so that we can load for model prediction on the test dataset.\n",
    "\n",
    "We named the model `unet_model` and it will be stored in the same relative directory we are working on.\n",
    "\n",
    "We will then load the model with the defined metrics for prediction on test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./unet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba35983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"./unet_model.h5\", custom_objects ={\"recall\":sm.metrics.Recall(threshold=0.5), \"f1\": f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23753d38",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329471ba",
   "metadata": {},
   "source": [
    "The final step here is model prediction on the test dataset.\n",
    "\n",
    "We will extract the unique ids for the test dataset, just like we did with the train dataset and store them into `test_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source_items = f\"{dataset_id}/{dataset_id}_source_test\"\n",
    "test_tiles = [clean_string(s) for s in next(os.walk(test_source_items))[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde8961",
   "metadata": {},
   "source": [
    "We will then obtain `test_tile_ids` which contains the field overall id excluding the timestamps e.g `10` from `10_2021_12`. Since we already have the list of timestamps in our variable `timestamps`, we only need the field id.\n",
    "\n",
    "We will then extract the source images (normalising, stacking bands) and store them into `X_test` while `loaded_tiles` tracks the id for each tile that we load in `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a880ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_ids = set()\n",
    "for tile in test_tiles:\n",
    "    test_tile_ids.add(tile.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29312c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.empty((len(test_tile_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS*len(timestamps)), dtype=np.float32)\n",
    "i = 0\n",
    "loaded_tiles = []\n",
    "for tile_id in test_tile_ids:\n",
    "    idx = 0\n",
    "    for timestamp in timestamps:\n",
    "        bd1 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B01.tif\")\n",
    "        bd1_array = bd1.read(1)\n",
    "        bd2 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B02.tif\")\n",
    "        bd2_array = bd2.read(1)\n",
    "        bd3 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B03.tif\")\n",
    "        bd3_array = bd3.read(1)\n",
    "        bd4 = rio.open(f\"{test_source_items}/{dataset_id}_source_test_{tile_id}_{timestamp}/B04.tif\")\n",
    "        bd4_array = bd4.read(1)\n",
    "        b01_norm = normalize(bd1_array)\n",
    "        b02_norm = normalize(bd2_array)\n",
    "        b03_norm = normalize(bd3_array)\n",
    "        b04_norm = normalize(bd4_array)\n",
    "        \n",
    "        field = np.dstack((b04_norm, b03_norm, b02_norm, b01_norm))\n",
    "        X_test[i][:,:,idx:idx+IMG_CHANNELS] = field\n",
    "        idx+=IMG_CHANNELS\n",
    "    loaded_tiles.append(str(tile_id).zfill(2)) #track order test tiles are loaded into X to make sure tile id matches \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9b70d",
   "metadata": {},
   "source": [
    "We will then use the model to predict the field boundaries from the time-series source imagery.\n",
    "The model's prediction is probabilistic i.e from 0 to 1, where 0 signifies that the model believes no field boundary exist for that particular pixel and 1 signifies the model is confident a field boundary definitely exists for the pixel. That is, a model prediction of 0.3 for a pixel signifies the model believes there is a 30% chance the pixel indicates a field boundary.\n",
    "\n",
    "Since the label images are either 0 or 1 (a field boundary doesn't exist or it does) for each pixel, We have to choose a threshold where probabilities below that threshold represent 0 (not field boundary) and probabilities above it represent 1 (field boundary). \n",
    "\n",
    "In this case, we will choose 0.5 as a fair threshold, which is seen in the below code.\n",
    "\n",
    "Also, we will reshape the prediction to fit our standard `IMG_HEIGHT * IMG_WIDTH` dimension e.g 256 x 256.\n",
    "\n",
    "We will then save the prediction into a dictionary where the key is the field id which we stored in `loaded_tiles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dictionary = {}\n",
    "for i in range(len(test_tile_ids)):\n",
    "    model_pred = model.predict(np.expand_dims(X_test[i], 0))\n",
    "    model_pred = model_pred[0]\n",
    "    model_pred = (model_pred >= 0.5).astype(np.uint8)\n",
    "    model_pred = model_pred.reshape(IMG_HEIGHT, IMG_WIDTH)\n",
    "    predictions_dictionary.update([(str(loaded_tiles[i]), pd.DataFrame(model_pred))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64829ce",
   "metadata": {},
   "source": [
    "We will then save the predictions as a `.csv` file as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33186bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for key, value in predictions_dictionary.items():\n",
    "    ftd = value.unstack().reset_index().rename(columns={'level_0': 'row', 'level_1': 'column', 0: 'label'})\n",
    "    ftd['tile_row_column'] = f'Tile{key}_' + ftd['row'].astype(str) + '_' + ftd['column'].astype(str)\n",
    "    ftd = ftd[['tile_row_column', 'label']]\n",
    "    dfs.append(ftd)\n",
    "\n",
    "sub = pd.concat(dfs)\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac12a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f\"./harvest_sample_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4d879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1456b81e0739e370391a1194b27bc839a9f5597c29befb4a579ec9fcd0f2acde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
