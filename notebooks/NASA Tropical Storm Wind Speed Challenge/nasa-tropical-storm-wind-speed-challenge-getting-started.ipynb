{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# How to use the Radiant MLHub API to browse and download the NASA Tropical Storm Wind Speed Competition Data\n",
    "\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the NASA Tropical Storm Wind Speed Competition dataset. Full documentation for the API is available at [docs.mlhub.earth](http://docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization,retrieve the items (the data contained within them) from those collections, and load the data into a dataframe.\n",
    "\n",
    "Each item in our collection is explained in json format compliant with STAC label extension definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "M. Maskey, R. Ramachandran, I. Gurung, B. Freitag, M. Ramasubramanian, J. Miller (2020) \"Tropical Cyclone Wind Estimation Competition Dataset\", Version 1.0, Radiant MLHub. \\[Date Accessed\\] [https://doi.org/10.34911/rdnt.xs53up](https://doi.org/10.34911/rdnt.xs53up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This notebook utilizes the [`radiant-mlhub` Python client](https://pypi.org/project/radiant-mlhub/) for interacting with the API and the [`pandas`](https://pandas.pydata.org/) library for compiling the data. If you are running this notebooks using Binder, then these dependency has already been installed. If you are running this notebook locally, you will need to install this yourself.\n",
    "\n",
    "See the official [`radiant-mlhub` docs](https://radiant-mlhub.readthedocs.io/) for more documentation of the full functionality of that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "### Create an API Key\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [mlhub.earth/profile](https://mlhub.earth/profile). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key(s), which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details). \n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "*If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = 'PASTE_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from radiant_mlhub import Dataset, Collection, client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Collections\n",
    "\n",
    "A Radiant MLHub *Dataset* is a group of related *Collections*. We can use the `Dataset.list` method to get a list of the available datasets as Python objects and inspect their `id` and `title` attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in Dataset.list():\n",
    "    print(f'{dataset.id}: ({dataset.title})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in the \"Tropical Cyclone Wind Estimation Competition\" dataset. We can fetch this dataset using its\n",
    "ID (`nasa_tropical_storm_competition`) and then use the `collections` property to list the source imagery and label collections associated with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.fetch('nasa_tropical_storm_competition')\n",
    "\n",
    "print('Source Imagery Collections\\n--------------------------')\n",
    "for collection in dataset.collections.source_imagery:\n",
    "    print(collection.id)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Label Collections\\n-----------------')\n",
    "for collection in dataset.collections.labels:\n",
    "    print(collection.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset has 2 collections containing source imagery for this dataset and 1 collection containing \n",
    "labels.\n",
    "\n",
    "The following cell gets the first item from each collection and prints the item ID, as well as a summary of the assets associated with the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(item, collection):\n",
    "    print(f'Collection: {collection.id}')\n",
    "    print(f'Item: {item[\"id\"]}')\n",
    "    print('Assets:')\n",
    "    for asset_name, asset in item.get('assets', {}).items():\n",
    "        print(f'- {asset_name}: {asset[\"title\"]} [{asset[\"type\"]}]')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "for collection in dataset.collections:\n",
    "    item = next(client.list_collection_items(collection.id, limit=1))\n",
    "    print_summary(item, collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items in the  `*train_labels` collection have a `\"labels\"` JSON asset containing wind speed labels for each source image. Items in the `*test_source` and `*train_source` collections have both a `\"features\"` JSON asset containing image features as JSON and an `\"image\"` JPEG asset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Assets\n",
    "\n",
    "In the following section, we download all JSON assets for both the `test` and `train` collections. ML Hub makes archives available that contain all the assets for a given collection. We will download these archives for the `nasa_tropical_storm_competition_train_labels` and `nasa_tropical_storm_competition_test_source` collections and then extract the items that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download to a data folder the current working directory\n",
    "# download_dir = Path('./data').resolve()\n",
    "\n",
    "# # Use this to download the the typical Mac user Downloads folder\n",
    "download_dir = Path('~/Downloads').expanduser().resolve()\n",
    "\n",
    "# # Use this to download to the typical Linux /tmp directory\n",
    "# download_dir = Path('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Extracting the archives takes a while so this cell may take 5-10 minutes to complete\n",
    "archive_paths = dataset.download(output_dir=download_dir)\n",
    "for archive_path in archive_paths:\n",
    "    print(f'Extracting {archive_path}...')\n",
    "    with tarfile.open(archive_path) as tfile:\n",
    "        tfile.extractall(path=download_dir)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into a Dataframe\n",
    "\n",
    "The cells below will load both the training and test items into dataframes, join the two, and sort the rows by the Image ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "train_source = 'nasa_tropical_storm_competition_train_source'\n",
    "train_labels = 'nasa_tropical_storm_competition_train_labels'\n",
    "\n",
    "jpg_names = glob(str(download_dir / train_source / '**' / '*.jpg'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "    \n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "    labels_path = str(jpg_path.parent / 'labels.json').replace(train_source, train_labels)\n",
    "\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "        \n",
    "    # Load the labels data\n",
    "    with open(labels_path) as src:\n",
    "        labels_data = json.load(src)\n",
    "\n",
    "    train_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "        int(labels_data['wind_speed'])\n",
    "    ])\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    np.array(train_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean', 'Wind Speed']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "test_source = 'nasa_tropical_storm_competition_test_source'\n",
    "\n",
    "jpg_names = glob(str(download_dir / test_source / '**' / '*.jpg'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "\n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "\n",
    "    test_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "    ])\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.array(test_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
