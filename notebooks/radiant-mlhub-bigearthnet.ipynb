{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "How to use the Radiant MLHub API to browse and download the BigEarthNet dataset\n",
    "=====\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API to download labels and source imagery for the BigEarthNet dataset. Full documentation for the API is available at [docs.mlhub.earth](docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization, see the list of available collections and datasets, and retrieve the items (the data contained within them) from those collections. \n",
    "\n",
    "Each item in our collection is explained in json format compliant with [STAC](https://stacspec.org/) [label extension](https://github.com/radiantearth/stac-spec/tree/master/extensions/label) definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Requirements and Contact Information\n",
    "----\n",
    "\n",
    "The BigEarthNet archive was constructed by the Remote Sensing Image Analysis [(RSiM)](https://www.rsim.tu-berlin.de/menue/remote_sensing_image_analysis_group/) Group and the Database Systems and Information Management [(DIMA)](https://www.dima.tu-berlin.de/menue/database_systems_and_information_management_group/?no_cache=1) Group at the Technische UniversitÃ¤t Berlin [(TU Berlin)](https://www.tu-berlin.de/menue/home/parameter/en/). This work is supported by the European Research Council under the ERC Starting Grant BigEarth and by the German Ministry for Education and Research as Berlin Big Data Center [(BBDC)](http://www.bbdc.berlin/home/).\n",
    "\n",
    "The BigEarthNet archive *requires* the a citation of the BigEarthNet paper whenever the archive is used. The citation for this paper is listed below along with contact information for inqueries about the archive and a PDF manual detailing the structure of the archive.\n",
    "\n",
    "Citation\n",
    "--\n",
    "G. Sumbul, M. Charfuelan, B. Demir, V. Markl, \"[BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding](http://bigearth.net/static/documents/BigEarthNet_IGARSS_2019.pdf)\", IEEE International Geoscience and Remote Sensing Symposium, pp. 5901-5904, Yokohama, Japan, 2019.\n",
    "\n",
    "\n",
    "\n",
    "Contact Information\n",
    "--\n",
    "* Website: [www.bigearth.net](www.bigearth.net)\n",
    "* Email: contact@bigearth.net\n",
    "* Manual: [http://bigearth.net/static/documents/BigEarthNetManual.pdf](http://bigearth.net/static/documents/BigEarthNetManual.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication\n",
    "-----\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key, which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "Copy the API key, and paste it in the box bellow.\n",
    "\n",
    "Click **Run** or press `SHIFT` + `ENTER` before moving on to run this first piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the requests module is required to access the API\n",
    "import requests\n",
    "\n",
    "# copy your API key from dashboard.mlhub.earth and paste it in the following\n",
    "API_KEY = 'PASTE_YOUR_API_KEY_HERE'\n",
    "API_BASE = 'https://api.radiant.earth/mlhub/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for data collections\n",
    "-----\n",
    "\n",
    "To see what training data is available, you will want to see the collections available through the API.\n",
    "\n",
    "A collection represents the top-most data level. Typically this means the data comes from the same source for the same geography. It might include different years or sub-geographies.\n",
    "\n",
    "To find data with specific parameters, see the [API documentation](http://docs.mlhub.earth/?python#the-feature-collections-in-the-dataset).\n",
    "\n",
    "To see the list, simply run the following cell. The returned list shows the collection id values, collection license, and data source citation (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all collections\n",
    "r = requests.get(f'{API_BASE}/collections?key={API_KEY}')\n",
    "h = r.json()\n",
    "collections = h['collections']\n",
    "\n",
    "# print the list of collections \n",
    "for c in collections:\n",
    "    print(f'ID:       {c[\"id\"]}\\nLicense:  {c.get(\"license\", \"N/A\")}\\nCitation: {c.get(\"sci:citation\", \"N/A\")}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the query properties\n",
    "----\n",
    "\n",
    "The BigEarthNet dataset is split into two collections, one which contains the labels and one which contains the source imagery. Labels link to their respective source imagery items so we will set our collection ID to `bigearthnet_v1_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collectionId = 'bigearthnet_v1_labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Items\n",
    "----\n",
    "\n",
    "The next cell contains the functions which page through the results and download the labels and source imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 # Required to download assets hosted on S3\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def download_s3(uri, path):\n",
    "    parsed = urlparse(uri)\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path[1:]\n",
    "    s3.download_file(bucket, key, os.path.join(path, key.split('/')[-1]))\n",
    "    print(f'Downloaded s3://{bucket}/{key}')\n",
    "    \n",
    "def download_http(uri, path):\n",
    "    parsed = urlparse(uri)\n",
    "    r = requests.get(uri)\n",
    "    f = open(os.path.join(path, parsed.path.split('/')[-1]), 'wb')\n",
    "    for chunk in r.iter_content(chunk_size=512 * 1024): \n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "    f.close()\n",
    "    print(f'Downloaded {uri}')\n",
    "\n",
    "def get_download_uri(uri):\n",
    "    r = requests.get(uri, allow_redirects=False)\n",
    "    return r.headers['Location']\n",
    "\n",
    "def download(href, path):\n",
    "    download_uri = get_download_uri(href)\n",
    "    parsed = urlparse(download_uri)\n",
    "    \n",
    "    if parsed.scheme in ['s3']:\n",
    "        download_s3(download_uri, path)\n",
    "    elif parsed.scheme in ['http', 'https']:\n",
    "        download_http(download_uri, path)\n",
    "\n",
    "def download_source_and_labels(item):\n",
    "    labels = item.get('assets').get('labels')\n",
    "    links = item.get('links')\n",
    "    \n",
    "    # Make the directory to download the files to\n",
    "    path = f'bigearthnet/{item[\"id\"]}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # Download the labels\n",
    "    download(labels['href'], path)\n",
    "    \n",
    "    #Download the source imagery\n",
    "    for link in links:\n",
    "        if link['rel'] != 'source':\n",
    "            continue\n",
    "        \n",
    "        r = requests.get(link['href'], params={'key': API_KEY})\n",
    "        for key, asset in r.json()['assets'].items():\n",
    "            download(asset['href'], path)\n",
    "            \n",
    "def get_items(uri, classes=None, cloud_and_shadow=None, seasonal_snow=None, max_items_downloaded=None, items_downloaded=0):\n",
    "    r = requests.get(uri, params={'key': API_KEY})\n",
    "    collection = r.json()\n",
    "    for feature in collection.get('features', []):\n",
    "        # Check if the item has one of the label classes we're interested in\n",
    "        matches_class = True\n",
    "        if classes is not None:\n",
    "            matches_class = False\n",
    "            for label_class in feature['properties'].get('labels', []):\n",
    "                if label_class in classes:\n",
    "                    matches_class = True\n",
    "                    break\n",
    "        \n",
    "        # Check if the item matches the cloud and shadows filter we specify\n",
    "        matches_clouds = True\n",
    "        if cloud_and_shadow is not None:\n",
    "            matches_clouds = feature['properties'].get('cloud_and_shadow', False) == cloud_and_shadow\n",
    "            \n",
    "        \n",
    "        # Check if the item matches the seasonal snow filter we specify\n",
    "        matches_snow = True\n",
    "        if seasonal_snow is not None:\n",
    "            matches_snow = feature['properties'].get('seasonal_snow', False) == seasonal_snow\n",
    "            \n",
    "        # If the item does not match all of the criteria we specify, skip it\n",
    "        if not matches_class or not matches_clouds or not matches_snow:\n",
    "            continue\n",
    "            \n",
    "        # Download the label and source imagery for the item\n",
    "        download_source_and_labels(feature)\n",
    "        \n",
    "        # Stop downloaded items if we reached the maximum we specify\n",
    "        items_downloaded += 1\n",
    "        if max_items_downloaded is not None and items_downloaded >= max_items_downloaded:\n",
    "            return\n",
    "        \n",
    "    # Get the next page if results, if available\n",
    "    for link in collection['links']:\n",
    "        if link['rel'] == 'next' and link['href'] is not None:\n",
    "            get_items(link['href'], classes=classes, cloud_and_shadow=cloud_and_shadow, seasonal_snow=seasonal_snow, max_items_downloaded=max_items_downloaded, items_downloaded=items_downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading all labels\n",
    "----\n",
    "\n",
    "This next cell will download all labels and source imagery contained in the BigEarthnet dataset. For demonstration purposes in this notebook, we limit the number of items downloaded to 1. You can remove the `max_items_downloaded` argument and the function will download all 590,326 labels and source imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_items(f'{API_BASE}/collections/{collectionId}/items?limit={limit}', max_items_downloaded=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering downloads based of labels\n",
    "----\n",
    "\n",
    "A likely scenario is you only want to download tiles which contain certain land cover classes or tiles which are not cloudy. First you'll need to know which land cover classes are contained in the dataset so you know which ones to filter on. The next cell will query the API and return the possible values for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f'{API_BASE}/collections/{collectionId}/items?limit=1&key={API_KEY}')\n",
    "label_classes = r.json()['features'][0]['properties']['label:classes']\n",
    "for label_class in label_classes:\n",
    "    print(f'\\nClasses for {label_class[\"name\"]}')\n",
    "    for c in sorted(label_class['classes']):\n",
    "        print(f'- {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering downloads on label classes, cloud and shadows, and seasonal snow\n",
    "----\n",
    "\n",
    "The labels in BigEarthNet have three properties.\n",
    "1) An array of land cover type classes contained in the tile\n",
    "2) Whether the tile contains cloud and cloud shadows\n",
    "3) Whether the tile has seasonal snow\n",
    "\n",
    "We can filter our download based off one or more of the properties.\n",
    "\n",
    "In this next cell we will download the first 5 tiles which contain either the `Coniferous forest` or `Rice fields` classes, do not contain clouds and cloud shadows, and do not contain seasonal snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_items(\n",
    "    f'{API_BASE}/collections/{collectionId}/items?limit={limit}',\n",
    "    classes=['Coniferous forest', 'Rice fields'],\n",
    "    cloud_and_shadow=False,\n",
    "    seasonal_snow=False,\n",
    "    max_items_downloaded=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
