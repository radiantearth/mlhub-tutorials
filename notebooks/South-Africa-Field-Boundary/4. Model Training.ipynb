{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc83caa0",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# South Africa Field Boundary Detection Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a07a29",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This notebook focuses on training the South Africa field boundary data using UNet-Agri. With UNet-Agri, we will build a UNet model and take the approach further by adding a pre-trained Visual Geometry Group-19 (VGG-19) encoder with it to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a252ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' #gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f122c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the data path\n",
    "from pathlib import Path\n",
    "downloads_path = str(Path().resolve())\n",
    "data_path =str(f\"{downloads_path}/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ff10a",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01814cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm #for checking time progress of notebook cell\n",
    "from itertools import chain\n",
    "#for reading and shaping images\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "#splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import * #load vgg for u-net backbone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8ed7d",
   "metadata": {},
   "source": [
    "### Data Preparation for Training\n",
    "\n",
    "We will then load the data as `X` (for images to be trained on) and `Y` (for masks).\n",
    "\n",
    "As we still had loads of augmented images in the training data from the previous step, we decided to use the saved training data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_path = data_path+\"/augmented/\" #augmented data from previous notebook\n",
    "train_frame_path = f\"{augmented_path}/train_frames\" #decided to use the train data for train and val as it is sufficient enough\n",
    "train_mask_path = f\"{augmented_path}/train_masks\"\n",
    "train_ids = os.listdir(train_frame_path)\n",
    "print(len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b97564",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256 \n",
    "IMG_HEIGHT = 256 \n",
    "IMG_CHANNELS = 3\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage') #neglect minor warnings\n",
    "seed = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf87d2",
   "metadata": {},
   "source": [
    "Having 23000+ images for model training is an overkill, so we'll just use 7000 images as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c074677",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8) #empty numpy array for storing images\n",
    "print('Getting and resizing train images... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    path = train_frame_path #for training & validation\n",
    "    imge = imread(path+\"/\" + id_ )[:,:,:IMG_CHANNELS]\n",
    "    #imge = resize(imge, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True) #can use this to resize into a different dimension\n",
    "   # imge = resize(imge, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    X[n] = imge\n",
    "    if n == 7000: #7000 images. can be adjusted to desired amount depending on computational limits\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b710aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "print('Getting and resizing train images and masks ... ')\n",
    "sys.stdout.flush()\n",
    "mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
    "    #mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "    path = train_mask_path\n",
    "    mask_ = imread(path+\"/\" + id_)/255.\n",
    "    mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n",
    "    mask = np.maximum(mask, mask_)\n",
    "#         mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "#                                       preserve_range=True), axis=-2)\n",
    "        \n",
    "    Y[n] = mask_\n",
    "    if n == 7000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[0:7000]\n",
    "Y=Y[0:7000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fcbf8",
   "metadata": {},
   "source": [
    "Calling `model.fit` automatically splits this data into training and validation data for model training. So all we need to do is split `X` and `Y` into `x, y` and `x_test, y_test` for training and test data respectively using a 20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, x_test, y, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise a sample from the train dataset\n",
    "imshow(x[3]*2)\n",
    "plt.show()\n",
    "imshow(y[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cc0de",
   "metadata": {},
   "source": [
    "### Building the U-Net Agri (U-Net with a pre-trained VGG19) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Setting up U-Net with pre-trained VGG19 #########\n",
    "\n",
    "#helpful resources to understand the model: https://arxiv.org/pdf/1505.04597.pdf\n",
    "#https://arxiv.org/pdf/2006.04868.pdf\n",
    "\n",
    "def squeeze_excite_block(inputs, ratio=8): #for feature re-calibration\n",
    "    #improves performance by not losing distinguishing features\n",
    "    init = inputs\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    #he-normal takes into account the non-linearity of relu activation function and sigmoid\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = Multiply()([init, se])\n",
    "    return x\n",
    "\n",
    "def conv_block(inputs, filters):\n",
    "    #performs feature extraction\n",
    "    x = inputs\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\")(x) #produces tensor of outputs\n",
    "    x = BatchNormalization()(x)#takes conv2d output and normalises them. faster\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder1(inputs): #vgg encoder for the unet\n",
    "    skip_connections = []\n",
    "\n",
    "    model = VGG19(include_top=False, weights='imagenet', input_tensor=inputs) #APPLYING VGG19 ENCODER\n",
    "    names = [\"block1_conv2\", \"block2_conv2\", \"block3_conv4\", \"block4_conv4\"]\n",
    "    for name in names:\n",
    "        skip_connections.append(model.get_layer(name).output)\n",
    "\n",
    "    output = model.get_layer(\"block5_conv4\").output\n",
    "    return output, skip_connections\n",
    "\n",
    "def decoder1(inputs, skip_connections): #u-net decoder\n",
    "    num_filters = [256, 128, 64, 32]\n",
    "    skip_connections.reverse()\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
    "        x = Concatenate()([x, skip_connections[i]])\n",
    "        x = conv_block(x, f)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder2(inputs):\n",
    "    num_filters = [32, 64, 128, 256]\n",
    "    skip_connections = []\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = conv_block(x, f)\n",
    "        skip_connections.append(x)\n",
    "        x = MaxPool2D((2, 2))(x)\n",
    "\n",
    "    return x, skip_connections\n",
    "\n",
    "def decoder2(inputs, skip_1, skip_2):\n",
    "    num_filters = [256, 128, 64, 32]\n",
    "    skip_2.reverse()\n",
    "    x = inputs\n",
    "\n",
    "    for i, f in enumerate(num_filters):\n",
    "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
    "        x = Concatenate()([x, skip_1[i], skip_2[i]])\n",
    "        x = conv_block(x, f)\n",
    "\n",
    "    return x\n",
    "\n",
    "def output_block(inputs):\n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    return x\n",
    "\n",
    "def Upsample(tensor, size):\n",
    "    \"\"\"Bilinear upsampling\"\"\"\n",
    "    def _upsample(x, size):\n",
    "        return tf.image.resize(images=x, size=size)\n",
    "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
    "\n",
    "def ASPP(x, filter):\n",
    "    shape = x.shape\n",
    "\n",
    "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
    "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
    "    y1 = BatchNormalization()(y1)\n",
    "    y1 = Activation(\"relu\")(y1)\n",
    "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
    "\n",
    "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
    "    y2 = BatchNormalization()(y2)\n",
    "    y2 = Activation(\"relu\")(y2)\n",
    "\n",
    "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
    "    y3 = BatchNormalization()(y3)\n",
    "    y3 = Activation(\"relu\")(y3)\n",
    "\n",
    "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Activation(\"relu\")(y4)\n",
    "\n",
    "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
    "    y5 = BatchNormalization()(y5)\n",
    "    y5 = Activation(\"relu\")(y5)\n",
    "\n",
    "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
    "\n",
    "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation(\"relu\")(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def build_model(shape):\n",
    "    inputs = Input(shape)\n",
    "    x, skip_1 = encoder1(inputs)\n",
    "    x = ASPP(x, 64)\n",
    "    x = decoder1(x, skip_1)\n",
    "    outputs1 = output_block(x)\n",
    "\n",
    "    x = inputs * outputs1\n",
    "\n",
    "    x, skip_2 = encoder2(x)\n",
    "    x = ASPP(x, 64)\n",
    "    x = decoder2(x, skip_1, skip_2)\n",
    "    outputs2 = output_block(x)\n",
    "    outputs = Concatenate()([outputs1, outputs2])\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((256, 256, 3)) #IMAGE SIZE, build model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using iouscore, f1score, recall and precision metrics for our model\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),\n",
    "          sm.metrics.Recall(threshold=0.5), sm.metrics.Precision(threshold=0.5)]  #applied the threshold\n",
    "#loss=sm.losses.dice_loss + sm.losses.bce_jaccard_loss\n",
    "model.compile(optimizer='adam', loss=sm.losses.binary_crossentropy+sm.losses.dice_loss, metrics=metrics)#dice loss and binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ba506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12567278",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#earlystopper = EarlyStopping(patience=3, verbose=1) #stopper after number of epochs to end if no improvements, did not apply initially\n",
    "checkpointer = ModelCheckpoint(f\"{data_path}/unet_agri_256x256.h5\", verbose=1, save_best_only=True) #save best result as model\n",
    "history = model.fit(x=x_train, y=y_train,\n",
    "              validation_data=(x_val, y_val),\n",
    "              steps_per_epoch = len(x_)//batch_size,\n",
    "              validation_steps = len(x_val)//batch_size,\n",
    "              batch_size=batch_size, epochs=40, callbacks=[callbacks=[checkpointer]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb74e4",
   "metadata": {},
   "source": [
    "### Plot training results on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Plot training results ##############\n",
    "#training results\n",
    "loss = results.history['loss'] \n",
    "iou_score = results.history['iou_score']\n",
    "f1_score = results.history['f1-score']\n",
    "precision = results.history['precision']\n",
    "recall = results.history['recall']\n",
    "\n",
    "#validation results\n",
    "val_loss = results.history['val_loss']\n",
    "val_iou_score = results.history['val_iou_score']\n",
    "val_f1_score = results.history['val_f1-score']\n",
    "val_precision = results.history['val_precision']\n",
    "val_recall = results.history['val_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, iou_score, 'bo', label='Training iou score')\n",
    "plt.plot(epochs, val_iou_score, 'b', label='Validation iou score')\n",
    "plt.title('Training and validation iou score')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, f1_score, 'bo', label='Training f1 score')\n",
    "plt.plot(epochs, val_f1_score, 'b', label='Validation f1 score')\n",
    "plt.title('Training and validation f1 score')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, precision, 'bo', label='Training precision score')\n",
    "plt.plot(epochs, val_precision, 'b', label='Validation precision score')\n",
    "plt.title('Training and validation precision score')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, recall, 'bo', label='Training recall score')\n",
    "plt.plot(epochs, val_recall, 'b', label='Validation recall score')\n",
    "plt.title('Training and validation recall score')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
