{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "# Downloading and Training a Model using the Radiant MLHub Python Client\n",
    "\n",
    "In this notebook, we will run through the process of using the [`radiant-mlhub` Python client](https://pypi.org/project/radiant-mlhub/) to download a dataset from Radiant MLHub and train a basic cloud detection model. The purpose of this notebook is not to show how to develop a good model but instead to show how with just a relatively little amount of code we can have a basic model ready for inferencing.\n",
    "\n",
    "Radiant MLHub uses [STAC](https://stacspec.org/) standard for cataloging training datasets. Each item in our collections are explained in json format compliant with STAC [label extension](https://github.com/radiantearth/stac-spec/tree/master/extensions/label) definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication\n",
    "===\n",
    "### Create an API Key\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [mlhub.earth](https://mlhub.earth/profile). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **Settings and Profile** section, you'll be able to create API key(s), which you will need. Do not share your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details).\n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from radiant_mlhub import Dataset\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urljoin\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from rasterio.plot import show\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = 'YOUR API KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Training Data AoI\n",
    "===\n",
    "\n",
    "I've gone ahead and preselected an AoI which we will use for filtering our training data. We will download any of the training data which falls within this small region around Kilimanjaro National Park in Tanzania. You can visualize the area this AoI covers at this [GitHub Gist link](https://gist.github.com/kbgg/7ad5b5a2e4eee3f86008409deae6a311)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = {\n",
    "  \"type\": \"Feature\",\n",
    "  \"properties\": {},\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [\n",
    "          36.990966796875,\n",
    "          -3.4640741915530184\n",
    "        ],\n",
    "        [\n",
    "          37.5457763671875,\n",
    "          -3.4640741915530184\n",
    "        ],\n",
    "        [\n",
    "          37.5457763671875,\n",
    "          -2.9156109787373894\n",
    "        ],\n",
    "        [\n",
    "          36.990966796875,\n",
    "          -2.9156109787373894\n",
    "        ],\n",
    "        [\n",
    "          36.990966796875,\n",
    "          -3.4640741915530184\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading Dataset Assets\n",
    "===\n",
    "After browsing through the [datasets page](https://mlhub.earth/datasets), we found the cloud cover dataset that we wish to train a model with. This dataset has the ID `ref_cloud_cover_detection_challenge_v1` and has four collections. There are two test/train collections for cloud cover labels and two test/train collections for Sentinel-2 source imagery.\n",
    "\n",
    "For this notebook we are just keeping it simple and only downloading a small region of data covered by the AoI which works out to around 83 chips. In the next cell, I selected 5 of these 83 chips which provides a good balance of cloudy / non-cloudy pixels for training.\n",
    "\n",
    "You might also notice that we are downloading only from the `test` collection. Typically in datasets hosted on Radiant MLHub, we don't split the datasets into train/test splits but since this dataset was first developed for a competition with a predefined train and test split we kept that convention when ingesting the dataset into Radiant MLHub. Practically there is no difference between using train or test chips here for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.fetch('ref_cloud_cover_detection_challenge_v1')\n",
    "my_filter = dict(\n",
    "    ref_cloud_cover_detection_challenge_v1_test_labels=['labels'],\n",
    "    ref_cloud_cover_detection_challenge_v1_test_source=['B02', 'B03', 'B04', 'B08'],\n",
    ")\n",
    "dataset.download(intersects=aoi, collection_filter=my_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data Loader\n",
    "===\n",
    "\n",
    "Once the previous cell completes, all of the files matching the filters are downloaded locally into the same directory that this notebook is running within. This cell then locates all of the label and source rasters and loads the pixel samples into a custom PyTorch DataLoader. On the third to last line, we select the first five chips which provide a good balance of cloudy and non-cloudy pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def label_item_stac_files(labels_directory):\n",
    "    label_stac_files = []\n",
    "    files = glob.glob(f'{labels_directory}/**/*.json')\n",
    "    \n",
    "    for fname in files:\n",
    "        if os.path.exists(os.path.join(Path(fname).parent, 'labels.tif')):\n",
    "            label_stac_files.append(fname)\n",
    "            \n",
    "    return label_stac_files\n",
    "    \n",
    "def load_raster(fname):\n",
    "    with rasterio.open(fname) as src:\n",
    "        return src.read()[0].flatten(), src.profile\n",
    "\n",
    "@lru_cache(maxsize=32)\n",
    "def get_item_data(item_stac, assets=['B02.tif', 'B03.tif', 'B04.tif', 'B08.tif']):\n",
    "    labels, profile = load_raster(os.path.join(Path(item_stac).parent, 'labels.tif'))\n",
    "    \n",
    "    source_stac = item_stac.replace('labels', 'source')\n",
    "    source_samples = []\n",
    "    for asset in assets:\n",
    "        source, profile = load_raster(os.path.join(Path(source_stac).parent, asset))\n",
    "        source_samples.append(source)\n",
    "        \n",
    "    source_samples = np.array(source_samples).swapaxes(0, 1)\n",
    "    \n",
    "    return source_samples, labels, profile\n",
    "\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, label_stacs, label_dimension=(256,256)):\n",
    "        self.label_stacs = label_stacs\n",
    "        self.label_dimension = label_dimension       \n",
    "        self.cached_item = [None, None]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_stacs) * self.label_dimension[0] * self.label_dimension[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_idx = math.floor(idx / (self.label_dimension[0] * self.label_dimension[1]))\n",
    "        item_stac = self.label_stacs[item_idx]\n",
    "            \n",
    "        source_samples, labels, profile = get_item_data(item_stac)\n",
    "            \n",
    "        sample_index = idx - (item_idx * (self.label_dimension[0] * self.label_dimension[1]))\n",
    "        \n",
    "        return torch.FloatTensor(source_samples[sample_index].tolist()), torch.FloatTensor([int(labels[sample_index])])\n",
    "    \n",
    "train_label_items = label_item_stac_files('ref_cloud_cover_detection_challenge_v1/ref_cloud_cover_detection_challenge_v1_test_labels')[:5]\n",
    "training_data = ExampleDataset(label_stacs=list(train_label_items))\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Training Data Class Balance\n",
    "===\n",
    "\n",
    "Here we calculate the number of cloudy and non-cloudy pixels in our training dataset. We can see that there are around 40% more cloudy pixels but we'll go through the rest of the training process and see how the model performance with this class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7,7))\n",
    "\n",
    "samples = []\n",
    "\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    samples.extend(y.reshape(-1).detach().numpy().round().astype(np.uint16))\n",
    "    \n",
    "unique, counts = np.unique(samples, return_counts=True)\n",
    "ax.bar(['No Cloud', 'Cloud'], counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model Architecture\n",
    "===\n",
    "\n",
    "For this model we will classify each pixel individually. The four inputs are the `B02`, `B03`, `B04`, and `B08` Sentinel-2 pixel values and the output is either a `0` for not cloudy or `1` for cloudy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.hid1 = nn.Linear(4, 8)\n",
    "        self.hid2 = nn.Linear(8, 8)\n",
    "        self.oupt = nn.Linear(8, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        nn.init.zeros_(self.hid1.bias)\n",
    "        nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        nn.init.zeros_(self.hid2.bias)\n",
    "        nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hid1(x)) \n",
    "        x = torch.tanh(self.hid2(x))\n",
    "        x = torch.sigmoid(self.oupt(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch=1, total_epochs=5):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % batch_size * 100 == 0:\n",
    "            current_loss = loss.item()\n",
    "        \n",
    "        t.set_description_str(f'Epoch: {epoch}/{total_epochs}; Current Loss: {current_loss:0.2f}')\n",
    "        t.update(batch_size)\n",
    "        \n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "with tqdm(total=len(train_dataloader.dataset) * epochs) as t:\n",
    "    for e in range(epochs):\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, epoch=e+1, total_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually Validating Model Performance\n",
    "===\n",
    "\n",
    "After the model has been trained we can run an inference on a chip which the model has not been trained on and quickly validate the performance of the model visually. Here we can see that after being trained on even just 5 chips the model has some reasonable performance. Of course, if we were training this model for a production environment then we would train on a much larger subset or even the whole dataset as well as calculate accuracy metrics but for the purposes of this notebook this is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(item_stac, visualize=False):\n",
    "    source_samples, labels, profile = get_item_data(item_stac)\n",
    "    predictions = model(torch.FloatTensor(source_samples.tolist()))\n",
    "\n",
    "    predictions = np.flip(predictions.reshape(-1).detach().numpy().round().reshape(512,512), 0).astype(np.uint16)\n",
    "    source_samples = source_samples.reshape(512, 512, 4)#.swapaxes(2, 0).swapaxes(2,1)\n",
    "\n",
    "    if visualize:\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15, 5))\n",
    "        ax1.imshow(np.flip(source_samples[:, :, 1:], axis=2)/4000)\n",
    "        ax1.set_title(\"False Color NIR\")\n",
    "        ax1.axis('off')\n",
    "        ax2.imshow(np.flipud(predictions))\n",
    "        ax2.set_title(\"Cloud Prediction (Probabilistic)\")\n",
    "        ax2.axis('off')\n",
    "        ax3.imshow(np.reshape(labels, (512, 512)))\n",
    "        ax3.set_title(\"Cloud Labels\")\n",
    "        ax3.axis('off')\n",
    "        \n",
    "    return predictions.reshape(-1), labels\n",
    "\n",
    "test_labels = list(set(label_item_stac_files('ref_cloud_cover_detection_challenge_v1/ref_cloud_cover_detection_challenge_v1_test_labels')) - set(train_label_items))    \n",
    "pred, actual = inference(test_labels[11], visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Confusion Matrix\n",
    "===\n",
    "\n",
    "Here we produce a confusion matrix from all the test data and we can see that although the model does well at predicting pixels which are not cloudy, it does not perform well when a pixel is cloudy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for item in test_labels[:]:\n",
    "    outputs, labels = inference(item)\n",
    "    \n",
    "    y_pred.extend(outputs)\n",
    "    y_true.extend(labels) \n",
    "\n",
    "classes = ('No Cloud', 'Cloud')\n",
    "\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "class_totals = np.expand_dims(np.sum(cf_matrix, axis=1), axis=1)\n",
    "cf_matrix_norm = np.round(np.divide(cf_matrix, class_totals), decimals=4)\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix_norm, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,10))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "ylabel = plt.ylabel('True label')\n",
    "xlabel = plt.xlabel('Predicted label')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
